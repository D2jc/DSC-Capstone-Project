{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6394ce7e",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186a5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gzip\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305699a",
   "metadata": {},
   "source": [
    "# Reimplementation of DEHNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6007d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of DEHNN model\n",
    "class DEHNNLayer(nn.Module):\n",
    "    def __init__(self, node_in_features, edge_in_features):\n",
    "        super(DEHNNLayer, self).__init__()\n",
    "        # Transformation for aggregating edge features to update node features\n",
    "        self.node_mlp1 = nn.Sequential(nn.Linear(edge_in_features, edge_in_features), nn.ReLU())\n",
    "        # Transformation for aggregating node features to update sink nodes\n",
    "        self.edge_mlp2 = nn.Sequential(nn.Linear(node_in_features, node_in_features),nn.ReLU())\n",
    "        self.edge_mlp3 = nn.Sequential(nn.Linear(2 * node_in_features, 2 * node_in_features),nn.ReLU())\n",
    "\n",
    "        # Transformation for mapping node features to virtual nodes\n",
    "        self.node_to_virtual_mlp = nn.Sequential(nn.Linear(node_in_features, node_in_features),nn.ReLU())\n",
    "        # Transformation for updating higher virtual node features\n",
    "        self.virtual_to_higher_virtual_mlp = nn.Sequential(nn.Linear(node_in_features, edge_in_features),nn.ReLU())\n",
    "        # Transformation for updating virtual node features\n",
    "        self.higher_virtual_to_virtual_mlp = nn.Sequential(nn.Linear(edge_in_features, edge_in_features),nn.ReLU())\n",
    "        # Transformation for propagating virtual node features back to the original nodes\n",
    "        self.virtual_to_node_mlp = nn.Sequential(nn.Linear(edge_in_features, edge_in_features),nn.ReLU())\n",
    "\n",
    "        self.default_driver = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_sink_agg = nn.Parameter(torch.zeros(node_in_features))\n",
    "        self.default_edge_agg = nn.Parameter(torch.zeros(edge_in_features))\n",
    "        self.default_virtual_node = nn.Parameter(torch.zeros(node_in_features))\n",
    "        \n",
    "        # Initialize weights of the network layers\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Update node features by aggregating incident edge features\n",
    "        updated_node_features = {}\n",
    "        for node in hypergraph.nodes:\n",
    "            incident_edges = hypergraph.get_incident_edges(node) # Get driver and sink nodes for each edge\n",
    "            if incident_edges:\n",
    "                # Aggregate features of edges incident to the node\n",
    "                agg_features = torch.sum(torch.stack([self.node_mlp1(edge_features[edge]) for edge in incident_edges]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_edge_agg\n",
    "            updated_node_features[node] = agg_features\n",
    "\n",
    "        # Updating the edge features by aggregating node features\n",
    "        updated_edge_features = {}\n",
    "        for edge in hypergraph.edges:\n",
    "            driver, sinks = hypergraph.get_driver_and_sinks(edge)\n",
    "\n",
    "            driver_feature = node_features[driver] if driver is not None else self.default_driver # Feature for driver node\n",
    "\n",
    "            if sinks:\n",
    "                # Aggregate features of sink nodes using edge_mlp2\n",
    "                sink_agg = torch.sum(torch.stack([self.edge_mlp2(node_features[sink]) for sink in sinks]), dim=0)\n",
    "            else:\n",
    "                sink_agg = self.default_sink_agg\n",
    "            # Concatenate driver and sink features, then update edge features using edge_mlp3\n",
    "            concatenated = torch.cat([driver_feature, sink_agg])\n",
    "            updated_edge_features[edge] = self.edge_mlp3(concatenated)\n",
    "\n",
    "        # Aggregates the virtual nodes\n",
    "        virtual_node_agg = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            assigned_nodes = [node for node in hypergraph.nodes if hypergraph.get_virtual_node(node) == virtual_node]\n",
    "            if assigned_nodes:\n",
    "                # Aggregate features of nodes assigned to this virtual node using node_to_virtual_mlp\n",
    "                agg_features = torch.sum(torch.stack([self.node_to_virtual_mlp(node_features[node]) for node in assigned_nodes]), dim=0)\n",
    "            else:\n",
    "                agg_features = self.default_virtual_node\n",
    "            virtual_node_agg[virtual_node] = agg_features\n",
    "\n",
    "        # Aggregate all virtual node features to form higher virtual node feature\n",
    "        higher_virtual_feature = torch.sum(\n",
    "            torch.stack([self.virtual_to_higher_virtual_mlp(virtual_node_agg[vn]) for vn in virtual_node_agg]), dim=0\n",
    "        )\n",
    "        # Propagate higher virtual node features to each virtual node\n",
    "        propagated_virtual_node_features = {}\n",
    "        for virtual_node in range(hypergraph.num_virtual_nodes):\n",
    "            propagated_virtual_node_features[virtual_node] = self.higher_virtual_to_virtual_mlp(higher_virtual_feature)\n",
    "        \n",
    "        # Update node features by adding propagated virtual node features\n",
    "        for node in hypergraph.nodes:\n",
    "            virtual_node = hypergraph.get_virtual_node(node) # Get the virtual node for each node\n",
    "            propagated_feature = self.virtual_to_node_mlp(propagated_virtual_node_features[virtual_node])\n",
    "            updated_node_features[node] += propagated_feature # Add propagated feature to the node's feature\n",
    "\n",
    "        return updated_node_features, updated_edge_features\n",
    "\n",
    "# DEHNN class reimplementation\n",
    "class DEHNN(nn.Module):\n",
    "    def __init__(self, num_layers, node_in_features, edge_in_features):\n",
    "        super(DEHNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Build the layers of the network\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DEHNNLayer(node_in_features, edge_in_features)) # Add DEHNNLayer to the model\n",
    "            node_in_features, edge_in_features = edge_in_features, node_in_features\n",
    "            edge_in_features *= 2\n",
    "\n",
    "        edge_in_features = edge_in_features // 2 # Restore the edge feature dimension\n",
    "        self.output_layer = nn.Sequential(nn.Linear(node_in_features, 2)) # Final output layer for predictions\n",
    "\n",
    "    def forward(self, node_features, edge_features, hypergraph):\n",
    "        # Pass through all layers of the network\n",
    "        for layer in self.layers:\n",
    "            node_features, edge_features = layer(node_features, edge_features, hypergraph)\n",
    "            \n",
    "        # Stack the final node features and pass them through the output layer to get predictions\n",
    "        final_node_features = torch.stack([node_features[node] for node in hypergraph.nodes], dim=0)\n",
    "        output = self.output_layer(final_node_features)\n",
    "        return output\n",
    "\n",
    "# Basic Hypergraph\n",
    "class Hypergraph:\n",
    "    def __init__(self, nodes, edges, driver_sink_map, node_to_virtual_map, num_virtual_nodes):\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.driver_sink_map = driver_sink_map\n",
    "        self.node_to_virtual_map = node_to_virtual_map\n",
    "        self.num_virtual_nodes = num_virtual_nodes\n",
    "\n",
    "    # Method to get incident edges for a given node\n",
    "    def get_incident_edges(self, node):\n",
    "        return [edge for edge in self.edges if node in self.driver_sink_map[edge][1] or node == self.driver_sink_map[edge][0]]\n",
    "    \n",
    "    # Method to get the driver and sink nodes for a given edge\n",
    "    def get_driver_and_sinks(self, edge):\n",
    "        return self.driver_sink_map[edge]\n",
    "    # Method to get the virtual node assigned to a given node\n",
    "    def get_virtual_node(self, node):\n",
    "        return self.node_to_virtual_map[node]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c1703",
   "metadata": {},
   "source": [
    "# Training Model with Preprocessed Xbar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15601c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 16.1228\n",
      "Epoch [2/10], Loss: 8.4986\n",
      "Epoch [3/10], Loss: 9.9067\n",
      "Epoch [4/10], Loss: 6.2397\n",
      "Epoch [5/10], Loss: 7.6678\n",
      "Epoch [6/10], Loss: 5.9226\n",
      "Epoch [7/10], Loss: 6.7864\n",
      "Epoch [8/10], Loss: 5.9236\n",
      "Epoch [9/10], Loss: 6.3125\n",
      "Epoch [10/10], Loss: 6.5801\n"
     ]
    }
   ],
   "source": [
    "file_indices = range(2, 9)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Training the model with the designs 2-8 with epoch of 10 (due to computing reasons)\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0  # Sum loss over all datasets for each epoch\n",
    "    \n",
    "    for i in file_indices:\n",
    "        # Load data for the current file\n",
    "        clean_data_dir = 'clean_data/'\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.driver_sink_map.pkl', 'rb') as f:\n",
    "            driver_sink_map = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.node_features.pkl', 'rb') as f:\n",
    "            node_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.net_features.pkl', 'rb') as f:\n",
    "            edge_features = pickle.load(f)\n",
    "        \n",
    "        with open(f'{clean_data_dir}{i}.congestion.pkl', 'rb') as f:\n",
    "            congestion = pickle.load(f)\n",
    "        \n",
    "        partition = np.load(f'{clean_data_dir}{i}.partition.npy')\n",
    "        \n",
    "        # Turn data into nodes and edges\n",
    "        node_features = {k: torch.tensor(v).float().to(device) for k, v in node_features.items()}\n",
    "        edge_features = {k: torch.tensor(v).float().to(device) for k, v in edge_features.items()}\n",
    "        \n",
    "        nodes = list(range(len(node_features)))\n",
    "        edges = list(range(len(edge_features)))\n",
    "        hypergraph = Hypergraph(nodes, edges, driver_sink_map, partition, 2)\n",
    "        \n",
    "        # Recompute class weights for the current design to try to handle class imbalance\n",
    "        num_congested = sum(value == 1 for value in congestion.values())\n",
    "        num_not_congested = sum(value == 0 for value in congestion.values())\n",
    "        total = num_congested + num_not_congested\n",
    "        \n",
    "        # Emphasizing congested class\n",
    "        weight_for_class_0 = total / (2 * num_not_congested) * 0.5 if num_not_congested > 0 else 1.0\n",
    "        weight_for_class_1 = total / (2 * num_congested) * 1.5 if num_congested > 0 else 1.0\n",
    "        class_weights = torch.tensor([weight_for_class_0, weight_for_class_1]).to(device)\n",
    "        \n",
    "        # Update criterion for the current design\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(node_features, edge_features, hypergraph)\n",
    "        \n",
    "        # Target labels (binary: 0 for not congested, 1 for congested)\n",
    "        target = torch.tensor(list(congestion.values())).to(device)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()  # Reset gradients after each batch\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/10], Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553532e",
   "metadata": {},
   "source": [
    "# Testing Model on Design 13 from XBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d0f2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the data on design 13\n",
    "\n",
    "clean_data_dir = 'clean_data/'\n",
    "i = 13\n",
    "with open(f'{clean_data_dir}{i}.driver_sink_map.pkl', 'rb') as f:\n",
    "    driver_sink_map = pickle.load(f)\n",
    "\n",
    "with open(f'{clean_data_dir}{i}.node_features.pkl', 'rb') as f:\n",
    "    node_features = pickle.load(f)\n",
    "\n",
    "with open(f'{clean_data_dir}{i}.net_features.pkl', 'rb') as f:\n",
    "    edge_features = pickle.load(f)\n",
    "\n",
    "with open(f'{clean_data_dir}{i}.congestion.pkl', 'rb') as f:\n",
    "    congestion = pickle.load(f)\n",
    "\n",
    "partition = np.load(f'{clean_data_dir}{i}.partition.npy')\n",
    "\n",
    "# Turn data into nodes and edges\n",
    "node_features = {k: torch.tensor(v).float().to(device) for k, v in node_features.items()}\n",
    "edge_features = {k: torch.tensor(v).float().to(device) for k, v in edge_features.items()}\n",
    "\n",
    "test_output = model(node_features, edge_features, Hypergraph(node_features, edge_features, driver_sink_map, partition, 2))\n",
    "\n",
    "out = test_output.detach().cpu().numpy()\n",
    "out = np.array([np.argmax(i) for i in out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a842e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7364080635308491\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84      5117\n",
      "           1       0.25      0.11      0.15      1431\n",
      "\n",
      "    accuracy                           0.74      6548\n",
      "   macro avg       0.52      0.51      0.50      6548\n",
      "weighted avg       0.67      0.74      0.69      6548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actual labels\n",
    "true_labels = np.array(list(congestion.values()))\n",
    "# Predicted labels\n",
    "predicted_labels = out\n",
    "# Classification Report\n",
    "class_report = classification_report(true_labels, predicted_labels)\n",
    "print('Accuracy: ' + str(np.mean(np.array(list(congestion.values())) == out)))\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8930efa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): DEHNNLayer(\n",
       "    (node_mlp1): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (edge_mlp2): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=14, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (edge_mlp3): Sequential(\n",
       "      (0): Linear(in_features=28, out_features=28, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (node_to_virtual_mlp): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=14, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (virtual_to_higher_virtual_mlp): Sequential(\n",
       "      (0): Linear(in_features=14, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (higher_virtual_to_virtual_mlp): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (virtual_to_node_mlp): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): DEHNNLayer(\n",
       "    (node_mlp1): Sequential(\n",
       "      (0): Linear(in_features=28, out_features=28, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (edge_mlp2): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (edge_mlp3): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (node_to_virtual_mlp): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (virtual_to_higher_virtual_mlp): Sequential(\n",
       "      (0): Linear(in_features=1, out_features=28, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (higher_virtual_to_virtual_mlp): Sequential(\n",
       "      (0): Linear(in_features=28, out_features=28, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (virtual_to_node_mlp): Sequential(\n",
       "      (0): Linear(in_features=28, out_features=28, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers # Viewing each layer in the trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
